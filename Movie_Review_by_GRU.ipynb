{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\study\\PYTHON\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb.data_dir = \"data/IMDB/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.6-tf'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the IMDB dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import download\n",
    "import glob\n",
    "\n",
    "########################################################################\n",
    "\n",
    "# Directory where you want to download and save the data-set.\n",
    "# Set this before you start calling any of the functions below.\n",
    "data_dir = \"data/IMDB/\"\n",
    "\n",
    "# URL for the data-set on the internet.\n",
    "data_url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# Private helper-functions.\n",
    "\n",
    "def _read_text_file(path):\n",
    "    \"\"\"\n",
    "    Read and return all the contents of the text-file with the given path.\n",
    "    It is returned as a single string where all lines are concatenated.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path, encoding='utf8') as file:\n",
    "        # Read a list of strings.\n",
    "        lines = file.readlines()\n",
    "\n",
    "        # Concatenate to a single string.\n",
    "        text = \" \".join(lines)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# Public functions that you may call to download the data-set from\n",
    "# the internet and load the data into memory.\n",
    "\n",
    "\n",
    "def maybe_download_and_extract():\n",
    "    \"\"\"\n",
    "    Download and extract the IMDB Review data-set if it doesn't already exist\n",
    "    in data_dir (set this variable first to the desired directory).\n",
    "    \"\"\"\n",
    "\n",
    "    download.maybe_download_and_extract(url=data_url, download_dir=data_dir)\n",
    "\n",
    "\n",
    "def load_data(train=True):\n",
    "    \"\"\"\n",
    "    Load all the data from the IMDB Review data-set for sentiment analysis.\n",
    "    :param train: Boolean whether to load the training-set (True)\n",
    "                  or the test-set (False).\n",
    "    :return:      A list of all the reviews as text-strings,\n",
    "                  and a list of the corresponding sentiments\n",
    "                  where 1.0 is positive and 0.0 is negative.\n",
    "    \"\"\"\n",
    "\n",
    "    # Part of the path-name for either training or test-set.\n",
    "    train_test_path = \"train\" if train else \"test\"\n",
    "\n",
    "    # Base-directory where the extracted data is located.\n",
    "    dir_base = os.path.join(data_dir, \"aclImdb\", train_test_path)\n",
    "\n",
    "    # Filename-patterns for the data-files.\n",
    "    path_pattern_pos = os.path.join(dir_base, \"pos\", \"*.txt\")\n",
    "    path_pattern_neg = os.path.join(dir_base, \"neg\", \"*.txt\")\n",
    "\n",
    "    # Get lists of all the file-paths for the data.\n",
    "    paths_pos = glob.glob(path_pattern_pos)\n",
    "    paths_neg = glob.glob(path_pattern_neg)\n",
    "\n",
    "    # Read all the text-files.\n",
    "    data_pos = [_read_text_file(path) for path in paths_pos]\n",
    "    data_neg = [_read_text_file(path) for path in paths_neg]\n",
    "\n",
    "    # Concatenate the positive and negative data.\n",
    "    x = data_pos + data_neg\n",
    "\n",
    "    # Create a list of the sentiments for the text-data.\n",
    "    # 1.0 is a positive sentiment, 0.0 is a negative sentiment.\n",
    "    y = [1.0] * len(data_pos) + [0.0] * len(data_neg)\n",
    "\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text, y_train=load_data(train=True)\n",
    "x_test_text, y_test=load_data(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text=x_train_text+x_test_text #total data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=10000)  #Taking top 10000 words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(data_text) #converted top 10000 textual words to number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tokens=tokenizer.texts_to_sequences(x_train_text) #applying on x_train words and converting it into number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  38,   14,  744, 3506,   45,   75,   32, 1771,   15,  153,   18,\n",
       "        110,    3, 1344,    5,  343,  143,   20,    1,  920,   12,   70,\n",
       "        281, 1228,  395,   35,  115,  267,   36,  166,    5,  368,  158,\n",
       "         38, 2058,   15,    1,  504,   88,   83,  101,    4,    1, 4339,\n",
       "         14,   39,    3,  432, 1148,  136, 8697,   42,  177,  138,   14,\n",
       "       2791,    1,  295,   20, 5276,  351,    5, 3029, 2310,    1,   38,\n",
       "       8697,   43, 3611,   26,  365,    5,  127,   53,   20,    1, 2032,\n",
       "          7,    7,   18,   48,   43,   22,   70,  358,    3, 2343,    5,\n",
       "        420,   20,    1, 2032,   15,    3, 3346,  208,    1,   22,  281,\n",
       "         66,   36,    3,  344,    1,  728,  730,    3, 3864, 1320,   20,\n",
       "          1, 1543,    3, 1293,    2,  267,   22,  281, 2734,    5,   63,\n",
       "         48,   44,   37,    5,   26, 4339,   12,    6, 2079,    7,    7,\n",
       "       3425, 2891,   35, 4446,   35,  405,   14,  297,    3,  986,  128,\n",
       "         35,   45,  267,    8,    1,  181,  366, 6951,    5,   94,    3,\n",
       "       2343,   16,    3, 7017, 3090,    5,   63,   43,   28,   67,  420,\n",
       "          8,    1, 2032,   15, 3082,  483,  208,    1,   43, 2802,   28,\n",
       "         67,   77,   48,   28,  487,   16,    3,  731, 1146,    4,  232,\n",
       "         51, 4161,    1,   20,  117,    6, 1334,   20,    1,  920,   16,\n",
       "          3,   20,   24, 4086,    5,   24,  170,  831,  117,   28,  185,\n",
       "       1562,  122,    1, 7951,  237,  358,    1,   31,    3,  100,   44,\n",
       "        407,   20,   24, 9597,  117,  911,   79,  102,  585,    3,  257,\n",
       "         31,    1,  389,    4, 5176, 2137, 4636,   32, 1222, 3303,   35,\n",
       "        189, 4287,  159, 2320,   40,  344,    2,   40, 8527, 6229, 1955,\n",
       "       4910,    2, 7720, 2618,   35,   23,  472,  328,    5,    1, 2032,\n",
       "        501, 4392,  213,  237,   21,  328,    5, 4805, 6768,   37,   28,\n",
       "        281,  115,   50,  109,  986,  117,   44,  557,   38, 2574,  505,\n",
       "         38,   26,  531,    7,    7,  136,    1,  112, 1906,  201, 5176,\n",
       "          2,  292, 1731,    5,  111,   10,  255,  114, 4541,    5,   26,\n",
       "         27,    4, 3425,  104,  117, 2557,    5,  109,    3,  202,    9,\n",
       "        276,    3, 4317,  486, 1107,    5,   24, 2347,  158,  138,   14,\n",
       "       8161,  186, 3889,   38,   15,    1,  504,    5,  119,   48,   44,\n",
       "         37,  263,  137, 4737,  159, 2320,    9,    1,  365,  254,   38,\n",
       "         20,    1,   79,  524,  232,    3,  364, 2343,   37,   29,  986,\n",
       "         83,   77,   50,   33,   89,  118,   48,    5,   77,   16,   65,\n",
       "        290,  273,   33,  142,  197,    9,    5,    1, 4339,  298,    4,\n",
       "        783,    9,   37,  290,    7,    7,   38,  273,   11,   19,   80,\n",
       "       5541,   22,    5,  343,  400])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_train_tokens[1]) #view of x_train review 1 in number format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_tokens=tokenizer.texts_to_sequences(x_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 291,  663,  164,  988, 6162, 1108,   53,   24, 2413, 2084,    1,\n",
       "       3368,  182,   16,   11,  236, 2845, 2378,  449,   42,    1, 1154,\n",
       "        580,  849,  117,    3,  186,  283, 5582,   36,   24, 4980,  952,\n",
       "          5,  288,  450,   24, 6506,    8,   48,   13, 2199,   14,    1,\n",
       "        811,  465,  123,  253,  145,   54,  326,    4, 5250,    2,  132,\n",
       "       6826, 2378, 1473,   23,    3, 9865,    3, 2579,   88, 1022,  221,\n",
       "          5, 1769,  928,   16, 5114,    2, 3673,  128,   18,   47,   85,\n",
       "         11,   19,   13, 8928,   29,    1,  167,    7,    7,    1,   19,\n",
       "        537,   16,   47, 1528,  615,  897,  816,    3,  318,    4,    1,\n",
       "       1270,  615,  897,    4,    2, 5049,   18,  341, 1371,   15,   92,\n",
       "         86,   31,    1, 1579,  527,  281,    1,  205, 1122,    5,    1,\n",
       "       1154,  580,  849,  177, 1185,   53,   52,   69, 6162,  124,    3,\n",
       "        331,  293,    2,  276,    3, 9513,   15, 1168, 3694,   10,  431,\n",
       "          1, 2790, 9673, 4106,    4,    1,  205,   20,  254,  105,    4,\n",
       "          1,  849,   12,    1,  111, 1069,   38,  726,   47, 2116, 2004,\n",
       "       1127,    5,    1, 3945,  222,   47, 1489,  108,  979,   50,    1,\n",
       "        703, 1448,    6, 2505,   31, 1133,    4,    1,    8,  317, 3244,\n",
       "          2,  342, 5563,   35, 2555,   24,  236,   14,    3,  534,    5,\n",
       "         94,   95,   15,    3, 5250,  265,   28,   81,  124,    3,   49,\n",
       "        293,    4, 2142, 4688,   48,  269,   20,    8,    1, 1795,  464,\n",
       "       4809, 5250,    3, 2336,  348, 4032,    6,  832, 7264, 1126,  133,\n",
       "       1098,  142,   81,   26,  358,    1,  268,    2, 2428, 2417,   15,\n",
       "       1835,   32, 1815,  829,  412,  851,    4, 1351,    2, 6035,   30,\n",
       "          1,  453,    4,    1, 9460, 1064,    7,    7,   22,  118,   85,\n",
       "         11,    6,  166,    5,  127,   21,   61,   84,   44,  441,   20,\n",
       "          3,  289,   64,   18,   81,   84,  104,    8,   11,  518,  827,\n",
       "          1,  167,  121,    2,  121,   18, 6162, 1401,   20,    3,  126,\n",
       "         71,  848,  119,    2,  387, 9742,   51,  634,  515,    1,  355,\n",
       "         71,   28,  123,   66,    8,  982,    4,    9,  467,    1, 4430,\n",
       "        926,   11,    6,    3,  331,    2,  766,   19,    5, 3766,   15,\n",
       "         12, 1003,    5,  165,   32,  309])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_test_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since the length of each review is not same so we have to pad and truncate them in order to get the same length \n",
    "#of the review so that it can be applied on the rnn and the embedding layer.\n",
    "#Let us pad and truncate both the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens=[len(token) for token in x_train_tokens+x_test_tokens]\n",
    "num_tokens=np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([127, 401, 134, ..., 253, 119, 352])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221.27716\n",
      "2209\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(num_tokens))\n",
    "print(np.max(num_tokens))\n",
    "#Such a large difference  ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens=np.mean(num_tokens)+2*np.std(num_tokens)\n",
    "max_tokens=int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9453"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets check how many reviews has word textlength<max tokens\n",
    "\n",
    "count=0\n",
    "for token in num_tokens:\n",
    "    if((token)<max_tokens):\n",
    "        count+=1\n",
    "    else:\n",
    "        continue\n",
    "print(count)\n",
    "token_whose_length_less_than_max_tokens=count/len(num_tokens)\n",
    "token_whose_length_less_than_max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#95 percent of the document has the length < max_tokens\n",
    "#let us pad the tokens to the fixed size input vecor for the GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pad=pad_sequences(x_train_tokens,maxlen=max_tokens,padding='pre',truncating='pre')\n",
    "x_test_pad=pad_sequences(x_test_tokens,maxlen=max_tokens,padding='pre',truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,   11,   13,    3, 1712,   17,   18,  111,   13,    3,\n",
       "        120,  843,    2,   88,    4,    1,  616,   70,   36,   47,    4,\n",
       "        953,   97,   16,   12,  301,    9,   13,  278,    1,   55,    5,\n",
       "        103,   10,  514,   11,    3,  447,   41,    4,  156,   34,  690,\n",
       "         92,   27,    4,  143,   97,   12,   22,   77,   21,  139,   41,\n",
       "          4,  125,   95,    5,   63,   18,   43,   22,  165,    9,   20,\n",
       "          1, 5809,  187,    3,  574])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pad[23457]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(input_dim=10000,output_dim=8,input_length=max_tokens,name='layer_embedding'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(GRU(units=16,return_sequences=True))\n",
    "model.add(GRU(units=16,return_sequences=True))\n",
    "model.add(GRU(units=32,return_sequences=True))\n",
    "model.add(GRU(units=64))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_embedding (Embedding)  (None, 544, 8)            80000     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 544, 16)           1200      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 544, 16)           1584      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 544, 32)           4704      \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 64)                18624     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 106,177\n",
      "Trainable params: 106,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/4\n",
      "22500/22500 [==============================] - 622s 28ms/step - loss: 0.3608 - acc: 0.8434 - val_loss: 0.2708 - val_acc: 0.9152\n",
      "Epoch 2/4\n",
      "22500/22500 [==============================] - 617s 27ms/step - loss: 0.3840 - acc: 0.8292 - val_loss: 0.5885 - val_acc: 0.7428\n",
      "Epoch 3/4\n",
      "22500/22500 [==============================] - 638s 28ms/step - loss: 0.2648 - acc: 0.8931 - val_loss: 0.3891 - val_acc: 0.8380\n",
      "Epoch 4/4\n",
      "22500/22500 [==============================] - 660s 29ms/step - loss: 0.2178 - acc: 0.9144 - val_loss: 0.2117 - val_acc: 0.9092\n",
      "25000/25000 [==============================] - 207s 8ms/step\n",
      "Accuracy: 86.79%\n",
      "Wall time: 45min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x_train_pad,y_train,validation_split=0.1,epochs=6,batch_size=64)\n",
    "accuracy_calculated=model.evaluate(x_test_pad,y_test)\n",
    "print(\"Accuracy: {0:.2%}\".format(accuracy_calculated[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Due to i3 processor it took a lot of time in my cpu .Will execute in the shorter span of time depending on the processor and ram "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
